{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Faster Style Transfer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook presents the **Faster Style Transfer** module usage steps and options. Emphasis was placed on the **inference/evaluation** part, and therefore on the actual images stylization. Model training guidelines are provided in the [GitHub repository](https://github.com/seloufian/Faster-Style-Transfer).\n",
        "\n",
        "This project implementation in based on the paper \"[A Learned Representation for Artistic Style](https://arxiv.org/abs/1610.07629)\" realized in 2016 by Google Brain.\n",
        "\n",
        "This style transfer technique, unlike the previous ones, uses a single Convolution Neural Network (CNN) trained on multiple styles, which offers a significant gain in storage memory and inference speed.\n",
        "\n",
        "To speed up the styling process, the notebook **prioritizes the GPU** (Nvidia Cuda) usage, and if not available, the CPU is used instead.\n",
        "\n",
        "Note that for it to run properly, the notebook **requires an internet connection** to load the illustration images, download model data and install required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grfLH2GJX3d8"
      },
      "outputs": [],
      "source": [
        "# Temporary: Unzip the module code.\n",
        "!unzip -q '/content/faster_style_transfer.zip'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOLI0xYpQ779"
      },
      "source": [
        "## Project and Data Download."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This section groups and structures all project components required by the application: the style transfer module, required packages and data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clone the project's GitHub repository and set it as the current working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone 'https://github.com/seloufian/Faster-Style-Transfer.git'\n",
        "\n",
        "%cd 'faster_style_transfer/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install required Python packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrvpAb1GQi8Z",
        "outputId": "41dfd0fa-f17c-46a8-9b24-007e62b698ba"
      },
      "outputs": [],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download the **artistic style images** on which the model was trained. These styles are used both by the model and for display/explanation purposes. Resized downloaded images can be found in this [Drive directory](https://drive.google.com/drive/folders/1CuKmsxpB7IH_1BdxJBYn0wx1oivJ9XqL), and the untouched original ones in this [Drive directory](https://drive.google.com/drive/folders/1o0iupQhGalGVA57mx7ZEb6vCrNLrp8Kc) (downloaded from [Google Arts](https://artsandculture.google.com/) using [Dezoomify](https://dezoomify.ophir.dev/))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bChi1aoiT9M"
      },
      "outputs": [],
      "source": [
        "!gdown -q --folder '1CuKmsxpB7IH_1BdxJBYn0wx1oivJ9XqL' -O 'data_style_transfer/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download the style transfer model best-results **checkpoint file** (25,026-th iteration), which contains the weights and other required metadata.\n",
        "During training, checkpoints were saved every 1,000 iterations, they range from 86 to 28,036 and are available in this [Drive directory](https://drive.google.com/drive/folders/1s3z789wocNvRlPUeVt6S9mpxrinPp2oz)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsijDfXriw1l"
      },
      "outputs": [],
      "source": [
        "# Checkpoint filename: checkpoint_25026.tar\n",
        "!gdown -q '1-ogTIuMNZ1XJj7rV_ckN80HrYTyYEBok' -O 'data_style_transfer/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download the **test images**, used for the stylization demo. Resized downloaded images can be found in this [Drive directory](https://drive.google.com/drive/folders/17s4zl1AFd4MP81_0pdzXJVWyQrVOpDN6), and the untouched original ones in this [Drive directory](https://drive.google.com/drive/folders/1_RWTd3BPVOENrEtn4hfH6WGiHCg2DPZG). In addition to these images, users can upload custom images for stylization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRSHvuAMWvom"
      },
      "outputs": [],
      "source": [
        "!gdown -q --folder '17s4zl1AFd4MP81_0pdzXJVWyQrVOpDN6' -O 'data_style_transfer/test/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ROduH4HRSi2"
      },
      "source": [
        "## Imports and Model Definition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import the model and required internal and external Python packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jc-w_ijDRcLE"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torchvision import io\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from faster_style_transfer.process import build_model, predict, visualize_images\n",
        "from faster_style_transfer.model import center_crop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build the **style transfer model** and restore its weights from the checkpoint file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "2f125a6743704b9da8cb630238559b9c",
            "e5c250248bee44fcbeb6d8cd165a6748",
            "32a64ec5159241da8b5c5791c8dc78dd",
            "6072a73d3b054d889d3d177f06b3a19a",
            "99d13db916eb4a3c817c1defa9bcf280",
            "88042b1a078d43059ab923d018dcbcba",
            "f287918df70a4f79a676574018170a0e",
            "50b4fdda41c143e19e9089d26542483c",
            "b20c4476700b4701b868d081539be0cf",
            "08fcb135107c4ac5a518c1d2b7bd59d8",
            "889a00b81eac4e2c956f9c5d5e6cab26"
          ]
        },
        "id": "VcJ4C8lPRcij",
        "outputId": "3a31961e-0cd6-4092-ee16-7492869186a0"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = build_model(\n",
        "    style_path='data_style_transfer/styles/',\n",
        "    checkpoint_path='data_style_transfer/checkpoint_25026.tar',\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELyJcf-wReN8"
      },
      "source": [
        "## Images Stylization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The project's style transfer model is based on a single Convolution Neural Network (CNN) trained on multiple styles. For its inference, it requires input image(s) to be stylized, and other parameters depending on the inference type: **Single Image Stylization** and **Weighted Image Stylization**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The list below contains the predefined test images. Although the model can process multiple images at once, in this demo, only one image is passed to the model. The test image can be modified just by uncommenting the chosen image path.\n",
        "In addition, users can upload their own images in *JPG* or *PNG* format and replace the `CONTENT_IMAGE_PATH` variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYgB95-3EQBS"
      },
      "outputs": [],
      "source": [
        "CONTENT_IMAGE_PATH = [\n",
        "    # 'data_style_transfer/test/resized/martyrs_memorial.jpg',\n",
        "    # 'data_style_transfer/test/resized/brad_pitt.jpg',\n",
        "    # 'data_style_transfer/test/resized/golden_gate_bridge.jpg',\n",
        "    # 'data_style_transfer/test/resized/women.jpg',\n",
        "    # 'data_style_transfer/test/resized/hoover_tower.jpg',\n",
        "    # 'data_style_transfer/test/resized/poznan_poland.jpg',\n",
        "    'data_style_transfer/test/resized/tubingen.jpg',\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZnUpSp9zT6L"
      },
      "source": [
        "To match the model's required input image size (**256 x 256**), uploaded images are automatically resized with **center cropping** to preserve their center content. The illustration below explains the *center-crop* process.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1_xSjyIIBk_0ZCDElOo5G2eMExfhUDn1M\" alt=\"Image center-crop process\" width=\"1024\" height=\"auto\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5zJkPrlRn2I"
      },
      "source": [
        "### Single Image Stylization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM-EPSKzxHAg"
      },
      "source": [
        "This stylization type applies a single style to an input image. The styles are identified by their index from 0 to 11 as the number of available styles which are shown in the illustration below.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1dVOv3RqFQvZ39Ljlz9Zt9_xx1rn0xruU\" alt=\"Available Styles\" width=\"1024\" height=\"auto\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The chosen style can be changed just by uncommenting its corresponding line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbQrd_G9zCBD"
      },
      "outputs": [],
      "source": [
        "SINGLE_STYLE = [\n",
        "    # (0, 'annunciation_virgin_deal.jpg'),\n",
        "    # (1, 'horses_on_the_seashore.jpg'),\n",
        "    # (2, 'the_scream.jpg'),\n",
        "    # (3, 'divan_japonais.jpg'),\n",
        "    # (4, 'portrait_of_pablo_picasso.jpg'),\n",
        "    # (5, 'three_fishing_boats.jpg'),\n",
        "    # (6, 'the_trial.jpg'),\n",
        "    # (7, 'great_wave_off_kanagawa.jpg'),\n",
        "    # (8, 'tullia_ride_body_chariot.jpg'),\n",
        "    # (9, 'head_of_a_clown.jpg'),\n",
        "    # (10, 'bicentennial_print.jpg'),\n",
        "    (11, 'the_starry_night.jpg'),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, the chosen image is stylized by the model. The output is shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "UQAudHDLzOMi",
        "outputId": "4c6853c8-6334-4917-f6e8-3d548ffa3f94"
      },
      "outputs": [],
      "source": [
        "# Styles are defined by their indexes.\n",
        "style_index = SINGLE_STYLE[0][0]\n",
        "\n",
        "# The model's prediction is a list of images.\n",
        "out_image = predict(model,\n",
        "    content_images_path=CONTENT_IMAGE_PATH,\n",
        "    style_index=style_index\n",
        ")\n",
        "\n",
        "out_image[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Weighted Image Stylization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This stylization type allows a combination of multiple (at least 2) styles. It requires a list of style indexes and their weights (decimals, from 0.0 to 1.0) which must sum to 1, so that they form a [convex combination](https://en.wikipedia.org/wiki/Convex_combination).\n",
        "\n",
        "Although the number of combined styles is not limited, in this demo, two types of combinations are considered: 2 and 4 styles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG6ib_mVRt7j"
      },
      "source": [
        "#### Weighted 2-Styles Inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This weighted stylization performs style transfer by combining **two** styles. For this, from the list below, exactly two styles should be selected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GFB6JtcR2pg"
      },
      "outputs": [],
      "source": [
        "WEIGHTED_TWO_STYLES = [\n",
        "    # (0, 'annunciation_virgin_deal.jpg'),\n",
        "    # (1, 1, 'horses_on_the_seashore.jpg'),\n",
        "    (2, 'the_scream.jpg'),\n",
        "    # (3, 'divan_japonais.jpg'),\n",
        "    # (4, 'portrait_of_pablo_picasso.jpg'),\n",
        "    # (5, 'three_fishing_boats.jpg'),\n",
        "    # (6, 'the_trial.jpg'),\n",
        "    # (7, 'great_wave_off_kanagawa.jpg'),\n",
        "    # (8, 'tullia_ride_body_chariot.jpg'),\n",
        "    # (9, 'head_of_a_clown.jpg'),\n",
        "    # (10, 'bicentennial_print.jpg'),\n",
        "    (11, 'the_starry_night.jpg'),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this demo, the combination weights are set to form a smooth transition from one style to another. The illustration below shows the sub-colors between two colors (the sides: left and right) by applying the combination weights.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=15oE6GSV5MflSACatRMi8XQRXO46wDEgV\" alt=\"2-Styles inference weights\" width=\"1024\" height=\"auto\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code below defines the weights, applies them to the previously defined test image, and displays the result in a grid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "WEIGHT_STEPS = [0, 0.25, 0.5, 0.75, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTv3oicvHwV6"
      },
      "outputs": [],
      "source": [
        "assert len(WEIGHTED_TWO_STYLES) == 2, 'Exactly TWO styles must be selected!'\n",
        "\n",
        "style_index = [style[0] for style in WEIGHTED_TWO_STYLES]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEfSw3LJR2-L"
      },
      "outputs": [],
      "source": [
        "sides, rows = [], []\n",
        "\n",
        "for weight_step in WEIGHT_STEPS:\n",
        "    weights = [weight_step, 1-weight_step]\n",
        "\n",
        "    out_image = predict(model,\n",
        "        content_images_path=CONTENT_IMAGE_PATH,\n",
        "        style_index=style_index,\n",
        "        weights=weights\n",
        "    )\n",
        "\n",
        "    if 0 in weights:\n",
        "        # The current image is one of the sides (left/right).\n",
        "        sides.append(out_image[0])\n",
        "    else:\n",
        "        # The current image is one of the in-between sub-columns.\n",
        "        rows.append(out_image[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "VBbeuwiAKKue",
        "outputId": "a6742361-4e41-456a-b52e-c4e78d58e09d"
      },
      "outputs": [],
      "source": [
        "visualize_images(sides, [rows])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Yz9IvKSR3Ov"
      },
      "source": [
        "#### Weighted 4-Styles Inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Same as the previous stylization except now **four** styles are combined. From the list below, exactly four styles should be chosen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9TH2nl-R6_J"
      },
      "outputs": [],
      "source": [
        "WEIGHTED_FOUR_STYLES = [\n",
        "    # (0, 'annunciation_virgin_deal.jpg'),\n",
        "    # (1, 1, 'horses_on_the_seashore.jpg'),\n",
        "    (2, 'the_scream.jpg'),\n",
        "    (3, 'divan_japonais.jpg'),\n",
        "    (4, 'portrait_of_pablo_picasso.jpg'),\n",
        "    # (5, 'three_fishing_boats.jpg'),\n",
        "    # (6, 'the_trial.jpg'),\n",
        "    # (7, 'great_wave_off_kanagawa.jpg'),\n",
        "    # (8, 'tullia_ride_body_chariot.jpg'),\n",
        "    # (9, 'head_of_a_clown.jpg'),\n",
        "    # (10, 'bicentennial_print.jpg'),\n",
        "    (11, 'the_starry_night.jpg'),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The defined combination weights form a smooth transition between the four styles. The illustration below shows the sub-colors between four colors (the 4 sides: upper-left, upper-right, lower-left, and lower-right) resulting from the weights application.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=17iDXTq20u-2J3F-FcAcM1WJsJZoTxLlk\" alt=\"4-Styles inference weights\" width=\"1024\" height=\"auto\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBJU1Y74YHn2"
      },
      "outputs": [],
      "source": [
        "WEIGHT_4_STYLES_STEPS = [\n",
        "    [[1, 0, 0, 0],\n",
        "     [0.75, 0.25, 0, 0],\n",
        "     [0.5, 0.5, 0, 0],\n",
        "     [0.25, 0.75, 0, 0],\n",
        "     [0, 1, 0, 0]],\n",
        "    [[0.75, 0, 0.25, 0],\n",
        "     [0.5625, 0.1875, 0.1875, 0.0625],\n",
        "     [0.375, 0.375, 0.125, 0.125],\n",
        "     [0.1875, 0.5625, 0.0625, 0.1875],\n",
        "     [0, 0.75, 0, 0.25]],\n",
        "    [[0.5, 0, 0.5, 0],\n",
        "     [0.375, 0.125, 0.375, 0.125],\n",
        "     [0.25, 0.25, 0.25, 0.25],\n",
        "     [0.125, 0.375, 0.125, 0.375],\n",
        "     [0, 0.5, 0, 0.5]],\n",
        "    [[0.25, 0, 0.75, 0],\n",
        "     [0.1875, 0.0625, 0.5625, 0.1875],\n",
        "     [0.125, 0.125, 0.375, 0.375],\n",
        "     [0.0625, 0.1875, 0.1875, 0.5625],\n",
        "     [0, 0.25, 0, 0.75]],\n",
        "    [[0, 0, 1, 0],\n",
        "     [0, 0, 0.75, 0.25],\n",
        "     [0, 0, 0.5, 0.5],\n",
        "     [0, 0, 0.25, 0.75],\n",
        "     [0, 0, 0, 1]],\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code defines the weights, applies them to the previously defined test image and displays the result in a grid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBnG4LaRR7N1"
      },
      "outputs": [],
      "source": [
        "assert len(WEIGHTED_FOUR_STYLES) == 4, 'Exactly FOUR styles must be selected!'\n",
        "\n",
        "style_index = [style[0] for style in WEIGHTED_FOUR_STYLES]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9Cz9_Xda4x-"
      },
      "outputs": [],
      "source": [
        "STYLES_DIR = Path('data_style_transfer/styles/')\n",
        "\n",
        "sides = []\n",
        "\n",
        "# Selected style images are loaded and resized to \"256 x 256\"\n",
        "# (like the model output), and added to the 4 sides of the grid.\n",
        "for _, style_name in WEIGHTED_FOUR_STYLES:\n",
        "    curr_style = Image.open(STYLES_DIR / style_name)\n",
        "    curr_style = curr_style.resize((256, 256))\n",
        "\n",
        "    sides.append(curr_style)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDTy6BvjMSiU"
      },
      "outputs": [],
      "source": [
        "rows = []\n",
        "\n",
        "# Loop over the 5 rows of the grid.\n",
        "for row_weights in WEIGHT_4_STYLES_STEPS:\n",
        "    row_images = []\n",
        "\n",
        "    # Loop over the 5 columns of the row.\n",
        "    # Each column is a combination of:\n",
        "    # - The upper and the lower rows (relative to the current row).\n",
        "    # - The columns on the left and on the right (relative the current column).\n",
        "    for weights in row_weights:\n",
        "        out_image = predict(model,\n",
        "            content_images_path=CONTENT_IMAGE_PATH,\n",
        "            style_index=style_index,\n",
        "            weights=weights\n",
        "        )\n",
        "\n",
        "        row_images.append(out_image[0])\n",
        "\n",
        "    rows.append(row_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "Y9u8jYizMSzI",
        "outputId": "cbe2974a-5bd7-439c-f093-5f55bbd9de27"
      },
      "outputs": [],
      "source": [
        "visualize_images(sides, rows)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "08fcb135107c4ac5a518c1d2b7bd59d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f125a6743704b9da8cb630238559b9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e5c250248bee44fcbeb6d8cd165a6748",
              "IPY_MODEL_32a64ec5159241da8b5c5791c8dc78dd",
              "IPY_MODEL_6072a73d3b054d889d3d177f06b3a19a"
            ],
            "layout": "IPY_MODEL_99d13db916eb4a3c817c1defa9bcf280"
          }
        },
        "32a64ec5159241da8b5c5791c8dc78dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50b4fdda41c143e19e9089d26542483c",
            "max": 553433881,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b20c4476700b4701b868d081539be0cf",
            "value": 553433881
          }
        },
        "50b4fdda41c143e19e9089d26542483c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6072a73d3b054d889d3d177f06b3a19a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08fcb135107c4ac5a518c1d2b7bd59d8",
            "placeholder": "​",
            "style": "IPY_MODEL_889a00b81eac4e2c956f9c5d5e6cab26",
            "value": " 528M/528M [00:02&lt;00:00, 229MB/s]"
          }
        },
        "88042b1a078d43059ab923d018dcbcba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "889a00b81eac4e2c956f9c5d5e6cab26": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99d13db916eb4a3c817c1defa9bcf280": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b20c4476700b4701b868d081539be0cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e5c250248bee44fcbeb6d8cd165a6748": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88042b1a078d43059ab923d018dcbcba",
            "placeholder": "​",
            "style": "IPY_MODEL_f287918df70a4f79a676574018170a0e",
            "value": "100%"
          }
        },
        "f287918df70a4f79a676574018170a0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
